{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# define line width and font size of plots\n",
    "plt.rcParams['lines.linewidth'] = 1.0\n",
    "plt.rcParams['font.size'] = 6.0\n",
    "plt.rcParams['axes.titlesize'] = 6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Definition\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25998 images belonging to 2 classes.\n",
      "Found 1560 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Load Images and Preprocessing (Resize, Scale, Batch Size Definition)\n",
    "img_size = 30\n",
    "batch_size = 32\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('train',\n",
    "                                                 target_size = (img_size, img_size),\n",
    "                                                 batch_size = batch_size,\n",
    "                                                 class_mode = 'binary',\n",
    "                                                 color_mode = 'rgb')\n",
    " \n",
    "test_set = test_datagen.flow_from_directory('test',\n",
    "                                            target_size = (img_size, img_size),\n",
    "                                            batch_size = batch_size,\n",
    "                                            class_mode = 'binary',\n",
    "                                            color_mode = 'rgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: (32, 30, 30, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO4AAAD1CAYAAABX/8l6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAADZRJREFUeJzt3VusXHUVx/Hfnj3TQxW5WYJFkIsVIiEGjcbglRBUBJVoIwgBJAElCCQSHhTQKBgTDREQjQ+KkWCCUENRExM0AQWJ+gBiQ8BShbZcJNxToLTnzJ7ZPrTydNbv39mn7ek6/X5eCF1nX2d+Z5pZXf9dtW0rALn05vsEAEyO4AIJEVwgIYILJERwgYQILpAQwd2NVVV13XyfA7rpz/cJYNtVVfVBSZ+UtFjSy5L+LOkkSd+TdJakIyVdK+lSSaslHSPpH5LeL+kCSb+QtFLSeyR9fes+l0i6QtLzkv7Vtu3KnXZB6Izg5vIFSau0JbjrJd0s6d1b/78n6RlJx0oaSbpe0pWSbpG0UdJhkja2bXtzVVVLJR2+dZ/HSWokPSXpoJ11IZgb/qqcywptCdxI0jmSTpB0mbaEst76M7Wkpt3yT+LGkoZb/9uTtLiqqvO37uOxrT9/t6QpSW+U9MBOuQrMWcU/edx9VFV1Xdu2X53v88DcEVwgIf6qDCREcIGECC6QEMEFEpqoj1tVFd9kzcGRh741rA363Vvq7vvFhx5d33m/mB9t21aln5noW2WCOzd33/j9sLZ0//0677dp4tpRp3yp834xP7YluPxVGUiI4AIJEVwgIYILJERwgYQILpAQwQUSIrhAQgQXSIjgAgkRXCAhggskRHCBhAgukBDBBRIiuEBCBBdIiOACCRFcIKHdds2pvfd8Q1gbDAZh7SdXXGD3+87DDg9rw2YU1tp2HNbGY3/bqypeoqiua1Pzv7f7g7j+0KPrwtrZl10d1jZtnrbHBGtOAQsWwQUSIrhAQgQXSIjgAgkRXCChBd0OOvygpWHtOxedHdaOXvb2sDY9E7d0JGlsvslvTDtobF6H0kvU68W/f93rW/d916Gu43rftJLWrH8irH3tmp/aYz7x9LO2vjugHQQsUAQXSIjgAgkRXCAhggskRHCBhFK3g9571Dts/ZIzl4e1ow4/NKzNDON9bh7GUzySNBqZKR9zr933/6OxP2bdjyeAbDvItJFK2/YH8TGnpuLamnXr7DFX3PGnsFaZ0/3P+qfC2sOPPm6PuauhHQQsUAQXSIjgAgkRXCAhggskRHCBhAgukNAu38c9/n3HhLWLTjvFbnvI0gPD2vQwHrFrxnEbzWwmSRqbnqtbydHe2MJdd/1hN/In+f5wZbrLPTPW1+/HtUHfHlJ77BH3gBctis/nwTWPhrVrblxpj3nfQ2v8Se1k9HGBBYrgAgkRXCAhggskRHCBhAgukFDhy/n5965l8UO03nZAvIqjJE3PxO2OaTOeNzYP0Woae0iZTVWYzov3WRq/G3VcPbJw3LoXX8zYtMWGdsVK3+kYm7NyK2wecehhYe3y80+3x7zuprhddM99D9pt5wufuEBCBBdIiOACCRFcICGCCyREcIGEdlo7aPnxHwpr53/25LA26E+Ftc0zvqHRmPLYLBnoWhZzGZCqzDEr10cq7tdM8XR8INiWescTGscbzpgWnSQNh3G/rTYTSeNxXDv4LQfZYx689ABTpR0EYDshuEBCBBdIiOACCRFcICGCCyS03dpBn/9Y3O6RpKvO/2JY27Q53m6TWZ1txvV75B+W5YZUKvP7rNS1cS2WHdW2GQwGnY45MlNFkl/4zm/npoP8tdR1vFic2vgeNUPTKioc8+KzTg1rr74Wvzl/e+e9dr87Ep+4QEIEF0iI4AIJEVwgIYILJERwgYQILpDQRA/9WrzHVLvsoNlXVnQP55Kk8z4Tj+5tmo77hWZhPzu2J0mtWTFwZB7A5X6fuQdhSb4f6/qi/uFcXtd+a2mU0L03XM2fj3/RKrOypGu+u2vp9/11Ti2O7/2eb4p75HvuGdfOu/wH9ph3/e2BsMZDv4AFiuACCRFcICGCCyREcIGECC6Q0ERjfcsOPlC/++G3Zq0NC6v3bXzVPZgq3s41D0rfmfttTWuhY0tC8q0Qt0qha2eU2jZ2W/vgLj/W5/bruoh+lczCA8zUrbXl2lOFy1QzNKtSTscbT8fdoM4tum3FJy6QEMEFEiK4QEIEF0iI4AIJEVwgoYlXeYy+5R6X2iSm7mdJzBTKHKaD3Nm632a2VSQ/5eO29R0ff8y67rbfUWk6yLwwdqjMvNbFWTSzkqN7Pf0kU+GQo/hCm5n4WoYz5lx3bDeIT1wgI4ILJERwgYQILpAQwQUSIrhAQpO1g9q4HVT8+tu1Hjq2QgqDJsWF3cLt7MO5/LauNdNzNbPjXqlt07FYupZRYx6aZto2bgKoKY3qGK7lM6f7Z/bbmHvgpoomWIOxEz5xgYQILpAQwQUSIrhAQgQXSIjgAgkRXCChifq4rVqNgj5c6TlFrpdmVyl0jchSs8zs16242HU1Rsn3lt2m/X7336GDOn4Z3WqDpZbq0JzweGTn+uJS+UltIf8+cdsVDtlxxUp3/yZ5mF4XfOICCRFcICGCCyREcIGECC6QEMEFEpqoHVSpUl3Xs9aKK+mZH/AP4DKjXGa0bMu2ca1nVlx0426lUcK+HQl0o2d+v06vMvd29pdri8L9a00rqTHHdF2m6P3z+jHN+yhqRUqFh2yNC61Kc0p29UhzyO9ecp495kwzM+uff+Xb19nt/o9PXCAhggskRHCBhAgukBDBBRIiuEBCE7WDhk2jp59/cdbaonrKbruoXhwXOw6aqC2MtxTaHSE7TuL7Xq6d4VpQrektuDaSJI3bJqxVph9UmZaOJPX78dujNfd+XJlaYTnQxjyAy7VmXJupcJl2v2PzZLmmia9z3732ssccLJr9NR0Mti2SfOICCRFcICGCCyREcIGECC6QEMEFEiK4QEIT9XEfWf+kPnrupbPWPnf8R+y23zz3nLDmnu5WVabvV+rPuVEv0xsdjeK+6KA0lmZOqjH7dT1eN84mlVYUjLd1fVpJGtuealxzvdpRoY/retZ2xM7dg8Isph+3NCOTduXS0iqPc1sFkk9cICGCCyREcIGECC6QEMEFEiK4QEITtYOc0sOwavNQq9qMTo3MQ5dKx/Qr9MVtCTvUZ1adLJ3TyIyBtaYdNC4soemOaUfW7IO7pKYxLR/zmrla+VlY3Vo+dpXHYuul23usZ9pM9z/0b3vEl17ZMOufv/zKRrvd68fepp8CsEshuEBCBBdIiOACCRFcICGCCyS03dpBvdq3ZgaD+HeEm0JpzAObxqWHVrkJFtcOMt2DptTPsBMjZjNzLX76R+raQpH8pE5r2kWuRWW6QdtwLeZ9Yls+sULX0LZ86tpMDpkW3g2//r095l/uX+VPqoBPXCAhggskRHCBhAgukBDBBRIiuEBC260ddE/h6+1b/3BnWFt+/HFhrW7ixdkasxCaJJnBIjsw4qZbRuXxlpBdXMz8Ci10M/zUjDvfwrW0bgLIbutaOqVJp26Ls7l7UJoic9xCcrfecVdYW7328c7H3BZ84gIJEVwgIYILJERwgYQILpAQwQUSIrhAQtutj/vMCy/Z+tU33RrWXts8E9bOOPETYa0prFLoWoZj0wO2ey22cV2v0ezWPmBrLr1jc8zSQ9O6PmTLjguWRjG7XavrtxZXIDWje2510lVr4pUcn3vR52Gu+MQFEiK4QEIEF0iI4AIJEVwgIYILJFRN8vV75WaudpCLT10e1s48+US77cisENmM4nZQY1aHLN0vO0rotjP7ret4tFGSRsOm035L19J1jK41N6F0LTuiHdTr+WP+bOXKsHb7XfHo3tDc91HHFSklqXU3cCs+cYGECC6QEMEFEiK4QEIEF0iI4AIJbbfpoB3lRytuC2ubZoZ22zNP+nhYW7RoEG84jtsHfnVDadh0awO43ZZWlhy77oGbSCp092rTYnH3oTKfB+Xhqm6tJNcO2jwTT59J0qubNsXbTvtt5wufuEBCBBdIiOACCRFcICGCCyREcIGEdvnpoLk448QTwto5nz4prO2z195hbTjygxvDcTx1NDILwo3NNElxIsls23XCR5Kq2rWZzAPMzDFd26bEbfvsSy+EtZ+v/I3d7x//+vfO57QjMB0ELFAEF0iI4AIJEVwgIYILJERwgYQILpDQgu7jOp/68LFh7eLT45Ull+y7xO53xoz1uZE/NybneryS7w+XHnjluPdGz+x3Ln3cXi/e9rGnngxrN9x2e1i794FV9pi7Gvq4wAJFcIGECC6QEMEFEiK4QEIEF0hot20HOR845uiwtmSffey2F54Wt5LevM9+Ya0xLR1Xk3y7aC6tIlfv2mbq1/6zYvXax8La9b9aEdb+uXpNp/PZFdEOAhYoggskRHCBhAgukBDBBRIiuEBCtIO2syMOOTisLZ6aCmvuxv78ym/YY7rOzKiJV518YcMGu98f3xI/cO2qC78c1lavXR/Wrv3lzfaYL254Oaytfeq/dtuFgnYQsEARXCAhggskRHCBhAgukBDBBRLqz/cJLDRr1j+x3fdZGsRxC7e1ZvG1pmnsfh9Zuy6s1Wa/Gze9Ftbuf3i1PSa2DZ+4QEIEF0iI4AIJEVwgIYILJERwgYQILpAQwQUSIrhAQgQXSIjgAgkRXCAhggskRHCBhCZd5fE5SfESfgDm6pC2bfcv/dBEwQWwa+CvykBCBBdIiOACCRFcICGCCyREcIGECC6QEMEFEiK4QEL/A5NzFtndM+lvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sanity Check\n",
    "print('The shape is (batch size, height, width, channels):',training_set[0][0].shape)\n",
    "\n",
    "# Pick and Plot a Random Example\n",
    "fig, axa = plt.subplots(1, 1)\n",
    "ax.set_title('example')\n",
    "ax.imshow(training_set[0][0][0])\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.savefig('example',dpi=900,transparent=True,orientation='landscape',bbox_inches='tight')\n",
    "plt.savefig('example',dpi=900,transparent=True,orientation='landscape',bbox_inches='tight',format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_20 (Conv2D)           (None, 30, 30, 5)         140       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 15, 15, 5)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 15, 15, 10)        460       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 8, 8, 10)          0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 640)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 641       \n",
      "=================================================================\n",
      "Total params: 1,241\n",
      "Trainable params: 1,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model Definition\n",
    "\n",
    "training_set_size = 25998\n",
    "test_set_size = 1560\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Convolution2D(5,(3,3),input_shape=(img_size,img_size,training_set[0][0].shape[3]),padding='same',data_format='channels_last',activation='relu'),\n",
    "    keras.layers.MaxPooling2D((2, 2), padding='same'),\n",
    "    keras.layers.Convolution2D(10,(3,3),padding='same',data_format='channels_last',activation='relu'),\n",
    "    keras.layers.MaxPooling2D((2, 2), padding='same'),\n",
    "    keras.layers.Flatten(),\n",
    "    #keras.layers.Dense(10, activation = 'relu'), # if you have a good CPU/GPU :S\n",
    "    keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(), \n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "813/812 [==============================] - 162s 199ms/step - loss: 0.6102 - acc: 0.6696 - val_loss: 0.5731 - val_acc: 0.6641\n",
      "Epoch 2/10\n",
      "813/812 [==============================] - 149s 184ms/step - loss: 0.4816 - acc: 0.7797 - val_loss: 0.3443 - val_acc: 0.8346\n",
      "Epoch 3/10\n",
      "813/812 [==============================] - 151s 186ms/step - loss: 0.3643 - acc: 0.8455 - val_loss: 0.2405 - val_acc: 0.8910\n",
      "Epoch 4/10\n",
      "813/812 [==============================] - 155s 191ms/step - loss: 0.2866 - acc: 0.8861 - val_loss: 0.1806 - val_acc: 0.9340\n",
      "Epoch 5/10\n",
      "813/812 [==============================] - 144s 177ms/step - loss: 0.2409 - acc: 0.9101 - val_loss: 0.1723 - val_acc: 0.9327\n",
      "Epoch 6/10\n",
      "813/812 [==============================] - 147s 181ms/step - loss: 0.2167 - acc: 0.9222 - val_loss: 0.1418 - val_acc: 0.9545\n",
      "Epoch 7/10\n",
      "813/812 [==============================] - 149s 184ms/step - loss: 0.1988 - acc: 0.9304 - val_loss: 0.1385 - val_acc: 0.9487\n",
      "Epoch 8/10\n",
      "813/812 [==============================] - 153s 188ms/step - loss: 0.1913 - acc: 0.9331 - val_loss: 0.1240 - val_acc: 0.9596\n",
      "Epoch 9/10\n",
      "813/812 [==============================] - 149s 184ms/step - loss: 0.1823 - acc: 0.9361 - val_loss: 0.1068 - val_acc: 0.9641\n",
      "Epoch 10/10\n",
      "813/812 [==============================] - 144s 177ms/step - loss: 0.1779 - acc: 0.9390 - val_loss: 0.1131 - val_acc: 0.9647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c4c51b320>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Fitting\n",
    "epochs = 5 # should lead to about 90% accuarcy\n",
    "\n",
    "model.fit_generator(training_set,\n",
    "                    steps_per_epoch = (training_set_size/batch_size),\n",
    "                    epochs = epochs,\n",
    "                    validation_data = test_set,\n",
    "                    validation_steps = (test_set_size/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

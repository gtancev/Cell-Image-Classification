{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# ML libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# define line width and font size of plots\n",
    "plt.rcParams['lines.linewidth'] = 1.0\n",
    "plt.rcParams['font.size'] = 6.0\n",
    "plt.rcParams['axes.titlesize'] = 6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Definition\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25998 images belonging to 2 classes.\n",
      "Found 1560 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Load Images and Preprocessing (Resize, Scale, Batch Size Definition)\n",
    "img_size = 30\n",
    "batch_size = 32\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('train',\n",
    "                                                 target_size = (img_size, img_size),\n",
    "                                                 batch_size = batch_size,\n",
    "                                                 class_mode = 'binary',\n",
    "                                                 color_mode = 'rgb')\n",
    " \n",
    "test_set = test_datagen.flow_from_directory('test',\n",
    "                                            target_size = (img_size, img_size),\n",
    "                                            batch_size = batch_size,\n",
    "                                            class_mode = 'binary',\n",
    "                                            color_mode = 'rgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape is (batch size, height, width, channels): (32, 30, 30, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 212.4x212.4 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sanity Check\n",
    "print('The shape is (batch size, height, width, channels):',training_set[0][0].shape)\n",
    "\n",
    "# Pick and Plot a Random Example\n",
    "fig, ax = plt.subplots(1, 1,figsize=(2.95,2.95))\n",
    "ax.set_title('example')\n",
    "ax.imshow(training_set[0][0][0])\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.savefig('example',dpi=900,transparent=True,orientation='landscape',bbox_inches='tight')\n",
    "plt.savefig('example',dpi=900,transparent=True,orientation='landscape',bbox_inches='tight',format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_20 (Conv2D)           (None, 30, 30, 5)         140       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 15, 15, 5)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 15, 15, 10)        460       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 8, 8, 10)          0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 640)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 641       \n",
      "=================================================================\n",
      "Total params: 1,241\n",
      "Trainable params: 1,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model Definition\n",
    "\n",
    "training_set_size = 25998\n",
    "test_set_size = 1560\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Convolution2D(5,(3,3),input_shape=(img_size,img_size,training_set[0][0].shape[3]),padding='same',data_format='channels_last',activation='relu'),\n",
    "    keras.layers.MaxPooling2D((2, 2), padding='same'),\n",
    "    keras.layers.Convolution2D(10,(3,3),padding='same',data_format='channels_last',activation='relu'),\n",
    "    keras.layers.MaxPooling2D((2, 2), padding='same'),\n",
    "    keras.layers.Flatten(),\n",
    "    #keras.layers.Dense(10, activation = 'relu'), # if you have a good CPU/GPU :S\n",
    "    keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(), \n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "813/812 [==============================] - 162s 199ms/step - loss: 0.6102 - acc: 0.6696 - val_loss: 0.5731 - val_acc: 0.6641\n",
      "Epoch 2/10\n",
      "813/812 [==============================] - 149s 184ms/step - loss: 0.4816 - acc: 0.7797 - val_loss: 0.3443 - val_acc: 0.8346\n",
      "Epoch 3/10\n",
      "813/812 [==============================] - 151s 186ms/step - loss: 0.3643 - acc: 0.8455 - val_loss: 0.2405 - val_acc: 0.8910\n",
      "Epoch 4/10\n",
      "813/812 [==============================] - 155s 191ms/step - loss: 0.2866 - acc: 0.8861 - val_loss: 0.1806 - val_acc: 0.9340\n",
      "Epoch 5/10\n",
      "813/812 [==============================] - 144s 177ms/step - loss: 0.2409 - acc: 0.9101 - val_loss: 0.1723 - val_acc: 0.9327\n",
      "Epoch 6/10\n",
      "813/812 [==============================] - 147s 181ms/step - loss: 0.2167 - acc: 0.9222 - val_loss: 0.1418 - val_acc: 0.9545\n",
      "Epoch 7/10\n",
      "813/812 [==============================] - 149s 184ms/step - loss: 0.1988 - acc: 0.9304 - val_loss: 0.1385 - val_acc: 0.9487\n",
      "Epoch 8/10\n",
      "813/812 [==============================] - 153s 188ms/step - loss: 0.1913 - acc: 0.9331 - val_loss: 0.1240 - val_acc: 0.9596\n",
      "Epoch 9/10\n",
      "813/812 [==============================] - 149s 184ms/step - loss: 0.1823 - acc: 0.9361 - val_loss: 0.1068 - val_acc: 0.9641\n",
      "Epoch 10/10\n",
      "813/812 [==============================] - 144s 177ms/step - loss: 0.1779 - acc: 0.9390 - val_loss: 0.1131 - val_acc: 0.9647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c4c51b320>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Fitting\n",
    "epochs = 5 # should lead to about 90% accuarcy\n",
    "\n",
    "model.fit_generator(training_set,\n",
    "                    steps_per_epoch = (training_set_size/batch_size),\n",
    "                    epochs = epochs,\n",
    "                    validation_data = test_set,\n",
    "                    validation_steps = (test_set_size/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
